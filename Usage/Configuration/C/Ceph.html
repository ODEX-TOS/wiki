<!--
    title: Ceph
    description: Migration of Ceph from the arch Wiki to the TOS Wiki
    published: true
    date: 2020-05-29T19:25:31.000Z
    tags: 
    -->>

<div id="content" class="mw-body" role="main" style="margin: 0">
	<a id="top"></a>
	
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading"  lang="en">Ceph</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" >From TOS Wiki</div>
		<div id="contentSub"></div>
		
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output">
<div class="archwiki-template-meta-related-articles-start">
<p>Related articles</p>
<ul>
<li><a href="/Usage/Configuration/C/Glusterfs.html" class="mw-redirect" title="GlusterFS">GlusterFS</a></li>
</ul>
</div>
<p>Ceph is a storage platform with a focus on being distributed, resilient, and having good performance and high reliability. Ceph can also be used as a block storage solution for <a href="https://en.wikipedia.org/wiki/Virtual_Machine"  title="wikipedia:Virtual Machine">virtual machines</a> or through the use of <a href="/Usage/Configuration/C/FUSE.html" title="FUSE">FUSE</a>, a conventional filesystem. Ceph is extremely configurable, with administrators being able to control virtually all aspects of the system. A command line interface is used to monitor and control the cluster. The platform also contains authentication &amp; authorization features, and various gateways to make it compatible with systems such as <a href="https://en.wikipedia.org/wiki/OpenStack#Swift"  title="wikipedia:OpenStack">OpenStack Swift</a> and <a href="https://en.wikipedia.org/wiki/Amazon_S3"  title="wikipedia:Amazon S3">Amazon S3</a>.
</p>
<p>From <a href="https://en.wikipedia.org/wiki/Ceph_(software)"  title="wikipedia:Ceph (software)">Wikipedia: Ceph (software)</a>:
</p>
<dl><dd>Ceph is a free software storage platform designed to present object, block, and file storage from a single distributed computer cluster. Ceph's main goals are to be completely distributed without a single point of failure, scalable to the exabyte level, and freely-available. The data is replicated, making it fault tolerant.</dd></dl>
<p>From <a rel="nofollow"  href="https://ceph.com/">Ceph.com</a>:
</p>
<dl><dd>Ceph is a distributed object store and file system designed to provide excellent performance, reliability and scalability.</dd></dl>
<div class="archwiki-template-box archwiki-template-box-warning">
<strong>Warning:</strong> The recommended installation method for Ceph is via an <a rel="nofollow"  href="https://github.com/ceph/ceph-deploy">upstream tool</a> which uses <a href="/Usage/Configuration/C/Secure_Shell.html" class="mw-redirect" title="SSH">SSH</a> to connect to machines with the purpose of automatically installing, configuring, and managing Ceph. The upstream tool (ceph-deploy) does not currently support <a href="/Usage/Configuration/C/TOS_Linux.html" title="TOS Linux">TOS Linux</a>. Until ceph-deploy includes support for TOS Linux, it is not possible to use the <a rel="nofollow"  href="https://docs.ceph.com/docs/master/start/">quick installation method</a><sup title="Last check status: 403">[<a href="https://en.wikipedia.org/wiki/Wikipedia:Link_rot"  title="wikipedia:Wikipedia:Link rot">dead link</a> 2020-03-28 ⓘ]</sup> due to the extensive use of the tool. The only other officially documented installation method is the <a rel="nofollow"  href="https://docs.ceph.com/docs/master/install/manual-deployment/">manual deployment guide</a>. This article therefore documents the manual procedure until TOS Linux is supported by the quick method.
<p>The official documentation <a rel="nofollow"  href="https://docs.ceph.com/docs/master/install/#deploy-a-cluster-manually">states</a> "the manual procedure is primarily for exemplary purposes for those developing deployment scripts with Chef, Juju, Puppet, etc.".
</p>
</div>
<div id="toc" >
<input type="checkbox" role="button" id="toctogglecheckbox"  style="display:none"><div  lang="en" dir="ltr">
<h2>Contents</h2>
<span ><label  for="toctogglecheckbox"></label></span>
</div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Terminology"><span >1</span> <span >Terminology</span></a></li>
<li class="toclevel-1 tocsection-2">
<a href="#Installation"><span >2</span> <span >Installation</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Packages"><span >2.1</span> <span >Packages</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#NTP_Client"><span >2.2</span> <span >NTP Client</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-5">
<a href="#Bootstrapping_a_storage_cluster"><span >3</span> <span >Bootstrapping a storage cluster</span></a>
<ul>
<li class="toclevel-2 tocsection-6"><a href="#Starting_a_monitor"><span >3.1</span> <span >Starting a monitor</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-7"><a href="#See_also"><span >4</span> <span >See also</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Terminology">Terminology</span></h2>
<div class="archwiki-template-box archwiki-template-box-note">
<strong>Note:</strong> A full glossary is available in the <a rel="nofollow"  href="https://docs.ceph.com/docs/master/glossary/">official documentation</a>.</div>
<ul>
<li>
<b>Client</b> : Something which connects to a Ceph cluster to access data but is not part of the Ceph cluster itself.</li>
<li>
<b>MONs</b> : Also known as monitors, these store cluster state and maps containing information about the cluster such as running services and data locations.</li>
<li>
<b>MDSs</b> : Also known as metadata servers, these store metadata for the Ceph filesystem to reduce load on the storage cluster (e.g. information for commands such as <code>ls</code>).</li>
<li>
<b>Node</b> : A machine which is running Ceph services, such as OSDs or MONs.</li>
<li>
<b>OSDs</b> : Also known as OSD daemons, these are responsible for the storage of data within the cluster and also conduct various related operations such as replication, recovery, and rebalancing.</li>
<li>
<b>Storage cluster</b> : The core set of software responsible for storing data (OSDs+MONs).</li>
</ul>
<h2><span class="mw-headline" id="Installation">Installation</span></h2>
<h3><span class="mw-headline" id="Packages">Packages</span></h3>
<p>Install it with the package <span class="plainlinks archwiki-template-pkg"><a rel="nofollow"  href="https://www.archlinux.org/packages/?name=ceph">ceph</a></span>, available in the <a href="/Usage/Configuration/C/Official_repositories.html" title="Official repositories">official repositories</a>. You may instead install <span class="plainlinks archwiki-template-pkg"><a rel="nofollow"  href="https://aur.archlinux.org/packages/ceph-git/">ceph-git</a></span><sup><small>AUR</small></sup> if you want a bleeding-edge installation.
</p>
<p>Install <span class="plainlinks archwiki-template-pkg"><a rel="nofollow"  href="https://www.archlinux.org/packages/?name=ceph">ceph</a></span> on all nodes that will be in the cluster.
</p>
<h3><span class="mw-headline" id="NTP_Client">NTP Client</span></h3>
<div class="archwiki-template-box archwiki-template-box-warning">
<strong>Warning:</strong> You should synchronise the clocks on your monitor nodes to prevent clock drift (see <a href="/Usage/Configuration/C/System_time.html#Time_skew" title="System time">System time#Time skew</a> for details), which can degrade the performance of your cluster or stop it from functioning entirely. The <a rel="nofollow"  href="https://docs.ceph.com/docs/master/rados/configuration/mon-config-ref/#clock">official documentation</a> recommends that nodes run some form of clock synchronisation.</div>
<p>Install and run a time synchronisation client on the node. See <a href="/Usage/Configuration/C/System_time.html#Time_synchronization" class="mw-redirect" title="Time synchronization">Time synchronization</a> for details.
</p>
<h2><span class="mw-headline" id="Bootstrapping_a_storage_cluster">Bootstrapping a storage cluster</span></h2>
<p>Before a storage cluster can operate, the monitors for that cluster must be bootstrapped with several identifiers and keyrings.
</p>
<p>The upstream Ceph documentation is well-written and kept updated with the latest releases.
</p>
<p>To boostrap a storage cluster, follow the steps documented in the <a rel="nofollow"  href="https://docs.ceph.com/docs/master/install/manual-deployment/#monitor-bootstrapping">official manual deployment guide</a>.
</p>
<h3><span class="mw-headline" id="Starting_a_monitor">Starting a monitor</span></h3>
<p>Since your system most likely uses <a href="/Usage/Configuration/C/Systemd.html" title="Systemd">systemd</a>, you can enable a monitor as a systemd unit.
</p>
<p>As an example, for a monitor named <code>node1</code> start and enable <code>ceph-mon@node1.service</code> as detailed in <a href="/Usage/Configuration/C/Systemd.html#Using_units" title="Systemd">Systemd#Using units</a>.
</p>
<h2><span class="mw-headline" id="See_also">See also</span></h2>
<ul>
<li>Official site
<ul>
<li><a rel="nofollow"  href="https://ceph.com">Homepage</a></li>
<li><a rel="nofollow"  href="https://docs.ceph.com/docs/master/">Documentation</a></li>
</ul>
</li>
<li>Official source code
<ul>
<li><a rel="nofollow"  href="https://github.com/ceph">GitHub organization</a></li>
<li><a rel="nofollow"  href="https://github.com/ceph/ceph">Ceph</a></li>
</ul>
</li>
</ul>
</div></div>
		
		<div id="catlinks"  data-mw="interface">
<div id="mw-normal-catlinks" class="mw-normal-catlinks">
<a href="../Special:Categories.html" title="Special:Categories">Categories</a>: <ul>
<li><a href="/Usage/Configuration/C/Category:File_systems.html" title="Category:File systems">File systems</a></li>
<li><a href="/Usage/Configuration/C/Category:Distributed_computing.html" title="Category:Distributed computing">Distributed computing</a></li>
<li><a href="/Usage/Configuration/C/Category:Red_Hat.html" title="Category:Red Hat">Red Hat</a></li>
</ul>
</div>
<div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden category: <ul><li><a href="/Usage/Configuration/C/Category:Pages_with_dead_links.html" title="Category:Pages with dead links">Pages with dead links</a></li></ul>
</div>
</div>
		<div ></div>
		
	</div>
</div>


		<div id="footer" role="contentinfo" style="margin: 0">
						<ul id="footer-info">
								<li>Retrieved from "<a dir="ltr" href="https://wiki.archlinux.org/index.php?title=Ceph&amp;oldid=602952">https://wiki.archlinux.org/index.php?title=Ceph&amp;oldid=602952</a>"</li>
		
		<li id="footer-info-lastmod"> This page was last edited on 28 March 2020, at 17:10.</li>
								<li id="footer-info-copyright">Content is available under <a  rel="nofollow" href="http://www.gnu.org/copyleft/fdl.html">GNU Free Documentation License 1.3 or later</a> unless otherwise noted.</li>
							<br>
</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="/Usage/Configuration/C/TOS Wiki:Privacy_policy.html" title="TOS Wiki:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="/Usage/Configuration/C/TOS Wiki:About.html" title="TOS Wiki:About">About TOS Wiki</a></li>
								<li id="footer-places-disclaimer"><a href="/Usage/Configuration/C/TOS Wiki:General_disclaimer.html" title="TOS Wiki:General disclaimer">Disclaimers</a></li>
							</ul>
										<ul id="footer-icons" >
										<li id="footer-copyrightico">
											</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		



